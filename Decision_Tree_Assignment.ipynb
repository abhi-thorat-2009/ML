{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical"
      ],
      "metadata": {
        "id": "uxs3WcUU8Y9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is a Decision Tree, and how does it work?\n",
        "\n",
        "A decision tree is a type of supervised machine learning algorithm that's used for both classification and regression tasks.  It's a tree-like structure where each internal node represents a \"decision\" or \"test\" on a feature (or attribute), each branch represents the outcome of the test, and each leaf node represents the final outcome (a class label or a predicted value).\n",
        "\n",
        "Here's a breakdown of how it works:\n",
        "\n",
        "**1. Building the Tree (Training Phase):**\n",
        "\n",
        "*   **Data Preparation:** The algorithm starts with a dataset of labeled examples (input features and corresponding target values).\n",
        "*   **Feature Selection:** The core of the algorithm is how it chooses which feature to split on at each node.  It aims to select the feature that best separates the data into more homogeneous subsets with respect to the target variable.  Common criteria for feature selection include:\n",
        "    *   **Gini Impurity (for classification):** Measures the impurity of a set of samples.  A lower Gini impurity indicates a more homogeneous set. The goal is to minimize the weighted average of the Gini impurities of the child nodes.\n",
        "    *   **Information Gain (for classification):** Measures the reduction in entropy (uncertainty) achieved by splitting the data on a particular feature.  The feature with the highest information gain is chosen.\n",
        "    *   **Mean Squared Error (for regression):** Measures the average squared difference between the predicted values and the actual values.  The goal is to minimize this error.\n",
        "*   **Splitting:** Once a feature is selected, the algorithm splits the data into subsets based on the possible values or ranges of that feature.  For categorical features, there's a branch for each category. For numerical features, the algorithm often finds an optimal threshold to split the data (e.g., values less than or equal to the threshold go down one branch, and values greater than the threshold go down the other).\n",
        "*   **Recursion:** This process of feature selection and splitting is repeated recursively for each subset until a stopping criterion is met.  Common stopping criteria include:\n",
        "    *   All samples in a subset belong to the same class (for classification).\n",
        "    *   A maximum tree depth is reached.\n",
        "    *   A minimum number of samples are required at a leaf node.\n",
        "    *   No more features are available to split on.\n",
        "*   **Leaf Node Assignment:** When a leaf node is reached, it's assigned a value.  For classification, this is the most frequent class among the samples in that leaf. For regression, it's typically the average (or median) of the target values of the samples in that leaf.\n",
        "\n",
        "**2. Making Predictions (Testing Phase):**\n",
        "\n",
        "*   **Input:** Given a new, unseen data point with its features.\n",
        "*   **Traversal:** The algorithm starts at the root node of the decision tree.\n",
        "*   **Decision at Each Node:** At each internal node, the algorithm checks the value of the corresponding feature in the input data point.  It then follows the branch corresponding to that value.\n",
        "*   **Leaf Node:** This process continues until a leaf node is reached.\n",
        "*   **Output:** The value assigned to the leaf node is the predicted outcome for the input data point (a class label for classification or a numerical value for regression).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_x2nasie8bA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7hvjSEH8QAh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. What are impurity measures in Decision Trees?\n",
        "\n",
        "In decision trees, impurity measures are crucial for determining the best way to split the data at each node. They quantify the \"disorder\" or \"mixed-up-ness\" of the classes within a set of data points. The goal is to find splits that reduce this impurity, creating subsets that are as \"pure\" as possible, meaning they contain mostly (or ideally, only) examples from a single class.\n",
        "\n",
        "Here's a breakdown of the two most common impurity measures:\n",
        "\n",
        "**1. Gini Impurity:**\n",
        "\n",
        "*   **What it measures:** The Gini impurity measures the probability of misclassifying a randomly chosen element in the dataset if it were labeled randomly according to the distribution of classes in the set.\n",
        "*   **How it works:**\n",
        "    *   For each class in the dataset, calculate the proportion of examples belonging to that class.\n",
        "    *   Square each of these proportions.\n",
        "    *   Sum up all the squared proportions.\n",
        "    *   Subtract this sum from 1.\n",
        "*   **Range:** Gini impurity ranges from 0 to 0.5.\n",
        "    *   0 means perfect purity (all examples belong to the same class).\n",
        "    *   0.5 means maximum impurity (classes are evenly distributed).\n",
        "*   **Advantages:**\n",
        "    *   Computationally simpler and faster than entropy.\n",
        "    *   Often preferred in practical implementations of decision trees.\n",
        "*   **Disadvantages:**\n",
        "    *   Slightly biased towards larger partitions.\n",
        "\n",
        "**2. Entropy:**\n",
        "\n",
        "*   **What it measures:** Entropy is a measure of the amount of uncertainty or disorder in a set of data. In the context of decision trees, it measures the uncertainty about the class labels in a node.\n",
        "*   **How it works:**\n",
        "    *   For each class in the dataset, calculate the proportion of examples belonging to that class.\n",
        "    *   Multiply each proportion by the logarithm base 2 of that proportion (with a negative sign).\n",
        "    *   Sum up all these values.\n",
        "*   **Range:** Entropy ranges from 0 to 1.\n",
        "    *   0 means perfect purity (all examples belong to the same class).\n",
        "    *   1 means maximum impurity (classes are evenly distributed).\n",
        "*   **Advantages:**\n",
        "    *   Considers the distribution of data more comprehensively than Gini impurity.\n",
        "    *   More commonly used in theoretical discussions and some decision tree algorithms (like C4.5 and ID3).\n",
        "*   **Disadvantages:**\n",
        "    *   More computationally intensive than Gini impurity.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WPZlN5xx8_Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6fkFLVOV9CX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. What is the mathematical formula for Gini Impurity?\n",
        "\n",
        "The mathematical formula for Gini Impurity is:\n",
        "\n",
        "```\n",
        "Gini Impurity = 1 - Σ (pᵢ)²\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "*   `pᵢ` is the proportion of samples belonging to class `i`.\n",
        "*   `Σ` represents the sum over all classes.\n",
        "\n",
        "Let's break it down:\n",
        "\n",
        "1.  **`pᵢ`:** For each class in your dataset, you calculate the proportion (or probability) of samples that belong to that class.  For example, if you have a dataset with two classes (positive and negative), and 60% of the samples are positive, then `p_positive = 0.6` and `p_negative = 0.4`.\n",
        "\n",
        "2.  **`pᵢ²`:** You square each of these proportions.  In the example above, `p_positive² = 0.6² = 0.36` and `p_negative² = 0.4² = 0.16`.\n",
        "\n",
        "3.  **`Σ (pᵢ)²`:** You sum up all the squared proportions.  In our example, `0.36 + 0.16 = 0.52`.\n",
        "\n",
        "4.  **`1 - Σ (pᵢ)²`:** Finally, you subtract the sum of squared proportions from 1.  In our example, `1 - 0.52 = 0.48`.  This is the Gini impurity for that node or subset of data.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say you have a node with 10 samples: 6 belong to class A and 4 belong to class B.\n",
        "\n",
        "1.  `p_A = 6/10 = 0.6`\n",
        "2.  `p_B = 4/10 = 0.4`\n",
        "3.  `p_A² = 0.36`\n",
        "4.  `p_B² = 0.16`\n",
        "5.  `Σ (pᵢ)² = 0.36 + 0.16 = 0.52`\n",
        "6.  `Gini Impurity = 1 - 0.52 = 0.48`\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "*   A Gini impurity of 0 means perfect purity (all samples belong to the same class).\n",
        "*   A Gini impurity of 0.5 (for a binary classification problem) means maximum impurity (classes are evenly distributed).  The closer the Gini impurity is to 0, the better the split (in terms of creating more homogeneous subsets).\n"
      ],
      "metadata": {
        "id": "lZVl_2fL9B4l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMw7qQXU9FS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. What is the mathematical formula for Entropy?\n",
        "\n",
        "The mathematical formula for entropy, specifically as used in decision trees (and information theory in general), is:\n",
        "\n",
        "```\n",
        "Entropy(S) = - Σ (pᵢ * log₂(pᵢ))\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "*   `Entropy(S)` represents the entropy of the set `S` (the set of data points at a particular node).\n",
        "*   `pᵢ` is the proportion of samples in `S` that belong to class `i`.\n",
        "*   `Σ` represents the sum over all classes.\n",
        "*   `log₂(pᵢ)` is the logarithm base 2 of `pᵢ`.\n",
        "\n",
        "Let's break it down step by step:\n",
        "\n",
        "1.  **`pᵢ`:**  For each class `i` in your dataset, calculate the proportion of samples that belong to that class within the current subset `S`.  For example, if you have a binary classification problem (positive and negative) and 70% of the samples in the current node are positive, then `p_positive = 0.7` and `p_negative = 0.3`.\n",
        "\n",
        "2.  **`log₂(pᵢ)`:** Take the base-2 logarithm of each `pᵢ`.  In our example, `log₂(0.7) ≈ -0.515` and `log₂(0.3) ≈ -1.737`.  You can use a calculator or programming language to compute these logarithms.\n",
        "\n",
        "3.  **`pᵢ * log₂(pᵢ)`:** Multiply each `pᵢ` by its corresponding `log₂(pᵢ)`. In our example, `0.7 * -0.515 ≈ -0.3605` and `0.3 * -1.737 ≈ -0.5211`.\n",
        "\n",
        "4.  **`- Σ (pᵢ * log₂(pᵢ))`:** Sum up all the values calculated in the previous step and then multiply the entire sum by -1.  In our example, `-(-0.3605 + -0.5211) ≈ 0.8816`. This is the entropy for that node or subset of data.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's say you have a node with 10 samples: 7 belong to class A and 3 belong to class B.\n",
        "\n",
        "1.  `p_A = 7/10 = 0.7`\n",
        "2.  `p_B = 3/10 = 0.3`\n",
        "3.  `log₂(0.7) ≈ -0.515`\n",
        "4.  `log₂(0.3) ≈ -1.737`\n",
        "5.  `p_A * log₂(p_A) ≈ -0.3605`\n",
        "6.  `p_B * log₂(p_B) ≈ -0.5211`\n",
        "7.  `Entropy(S) ≈ -(-0.3605 + -0.5211) ≈ 0.8816`\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "*   An entropy of 0 means perfect purity (all samples belong to the same class).\n",
        "*   An entropy of 1 (for a binary classification problem) means maximum impurity (classes are evenly distributed). The higher the entropy, the more \"mixed up\" the classes are in that node.  The goal when building a decision tree is to find splits that *reduce* entropy.\n",
        "\n"
      ],
      "metadata": {
        "id": "1m4vp3qt9Fnz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9tDqBVcN9IPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Information Gain is a key concept in decision tree algorithms. It's a measure of how much \"information\" a feature provides about the target variable. In simpler terms, it tells you how well a feature separates the data into different classes.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "**What it measures:**\n",
        "\n",
        "Information Gain measures the reduction in entropy (uncertainty) achieved by splitting the data on a particular feature. The feature with the highest information gain is chosen to split the data at each node of the decision tree.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1.  **Calculate the initial entropy:** First, calculate the entropy of the entire dataset before any split. This represents the initial uncertainty about the class labels.\n",
        "\n",
        "2.  **Calculate entropy after splitting:** For each feature, consider all possible ways to split the data based on that feature. Calculate the entropy of the subsets created by each split.\n",
        "\n",
        "3.  **Calculate weighted average entropy:** Calculate the weighted average of the entropies of the subsets. The weights are the proportions of samples that go into each subset.\n",
        "\n",
        "4.  **Calculate Information Gain:** Subtract the weighted average entropy of the subsets from the initial entropy. This gives you the information gain for that feature.\n",
        "\n",
        "5.  **Choose the best split:** Select the feature that has the highest information gain. This is the feature that best reduces the uncertainty about the class labels.\n",
        "\n",
        "**Mathematical Formula:**\n",
        "\n",
        "```\n",
        "Information Gain(S, A) = Entropy(S) - Σ (|Sv| / |S|) * Entropy(Sv)\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "*   `Information Gain(S, A)` is the information gain for splitting set `S` on attribute `A`.\n",
        "*   `Entropy(S)` is the entropy of the set `S` before the split.\n",
        "*   `|S|` is the number of samples in set `S`.\n",
        "*   `|Sv|` is the number of samples in subset `Sv` (the subset created by splitting on value `v` of attribute `A`).\n",
        "*   `Entropy(Sv)` is the entropy of the subset `Sv`.\n",
        "*   `Σ` represents the sum over all values `v` of attribute `A`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eGPYaUA09IsC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JW9JoFYK9Kmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Gini impurity and entropy are both measures of impurity used in decision trees to determine the best way to split the data at each node. While they both aim to quantify the \"disorder\" or \"mixed-up-ness\" of classes within a set of data points, they have some key differences:\n",
        "\n",
        "**1. Formula and Calculation:**\n",
        "\n",
        "*   **Gini Impurity:** `Gini Impurity = 1 - Σ (pᵢ)²`\n",
        "    *   It's calculated by squaring the proportion of samples belonging to each class, summing these squares, and subtracting the sum from 1.\n",
        "*   **Entropy:** `Entropy(S) = - Σ (pᵢ * log₂(pᵢ))`\n",
        "    *   It's calculated by multiplying the proportion of samples belonging to each class by the logarithm (base 2) of that proportion, summing these values, and then negating the sum.\n",
        "\n",
        "**2. Range of Values:**\n",
        "\n",
        "*   **Gini Impurity:** Ranges from 0 to 0.5.\n",
        "    *   0 means perfect purity (all samples belong to the same class).\n",
        "    *   0.5 means maximum impurity (classes are evenly distributed).\n",
        "*   **Entropy:** Ranges from 0 to 1.\n",
        "    *   0 means perfect purity.\n",
        "    *   1 means maximum impurity.\n",
        "\n",
        "**3. Computational Complexity:**\n",
        "\n",
        "*   **Gini Impurity:** Computationally simpler and faster to calculate because it involves only squaring and summing proportions.\n",
        "*   **Entropy:** More computationally intensive due to the use of logarithms, which can be slower to compute.\n",
        "\n",
        "**4. Sensitivity to Class Distribution:**\n",
        "\n",
        "*   **Entropy:** Considered to be slightly more sensitive to changes in class distribution, as it takes into account the logarithmic relationship between proportions and impurity.\n",
        "*   **Gini Impurity:** Slightly less sensitive to changes in class distribution.\n",
        "\n",
        "**5. Bias towards Larger Partitions:**\n",
        "\n",
        "*   **Gini Impurity:** Slightly biased towards creating larger partitions during splits.\n",
        "*   **Entropy:** Less biased in this regard.\n",
        "\n",
        "**6. Practical Use:**\n",
        "\n",
        "*   **Gini Impurity:** Often preferred in practical implementations of decision trees due to its computational efficiency.\n",
        "*   **Entropy:** More commonly used in theoretical discussions and some decision tree algorithms (like C4.5 and ID3).\n",
        "\n",
        "**In Summary:**\n",
        "\n",
        "*   Gini impurity is computationally faster and simpler, making it a good choice for large datasets.\n",
        "*   Entropy is slightly more accurate and considers the class distribution more comprehensively, but it's computationally more expensive.\n",
        "\n",
        "**Which one to use?**\n",
        "\n",
        "In most cases, the choice between Gini impurity and entropy doesn't significantly affect the performance of the decision tree. Gini impurity is often preferred due to its speed, but entropy might be preferred in some specific scenarios or for theoretical reasons. Many machine learning libraries allow you to choose either criterion when training decision trees.\n"
      ],
      "metadata": {
        "id": "KwQ7bCst9LB7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "INBFu1BI9NLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. What is the mathematical explanation behind Decision Trees?\n",
        "\n",
        " Here's a breakdown of the key mathematical concepts:\n",
        "\n",
        "**1. Impurity Measures (Gini Impurity & Entropy)**\n",
        "\n",
        "*   **Gini Impurity:**\n",
        "    *   Formula: `Gini Impurity = 1 - Σ (pᵢ)²`\n",
        "    *   `pᵢ` is the proportion of samples belonging to class `i`.\n",
        "    *   This measures how mixed up the classes are in a node. A pure node (all samples same class) has Gini = 0. A maximally impure node (equal class distribution) has Gini closer to 0.5 (for binary classification).\n",
        "*   **Entropy:**\n",
        "    *   Formula: `Entropy(S) = - Σ (pᵢ * log₂(pᵢ))`\n",
        "    *   `pᵢ` is the proportion of samples belonging to class `i`.\n",
        "    *   This measures the uncertainty in a node. A pure node has Entropy = 0. A maximally impure node has Entropy closer to 1 (for binary classification).\n",
        "\n",
        "**2. Information Gain**\n",
        "\n",
        "*   **Formula:** `Information Gain(S, A) = Entropy(S) - Σ (|Sv| / |S|) * Entropy(Sv)`\n",
        "    *   `S` is the set of samples at a node.\n",
        "    *   `A` is a feature we're considering to split on.\n",
        "    *   `Sv` are the subsets of `S` created by splitting on `A`.\n",
        "    *   `|S|` and `|Sv|` are the number of samples in those sets.\n",
        "    *   This measures how much splitting on feature `A` reduces the uncertainty. We want to maximize Information Gain.\n",
        "\n",
        "**3. Splitting Criteria**\n",
        "\n",
        "*   Decision trees use these impurity measures and Information Gain to decide the best feature to split on at each node.\n",
        "*   The algorithm calculates the Information Gain for all features and chooses the one with the highest gain. This means that feature best separates the data according to class labels.\n",
        "\n",
        "**4. Recursive Partitioning**\n",
        "\n",
        "*   This splitting process is repeated recursively for each child node until a stopping criterion is met:\n",
        "    *   All samples in a node belong to the same class (pure node).\n",
        "    *   Maximum tree depth is reached.\n",
        "    *   Minimum number of samples per leaf node is reached.\n",
        "\n",
        "**5. Mathematical Representation of the Tree**\n",
        "\n",
        "*   A decision tree can be seen as a piecewise constant function. Each leaf node represents a region in the feature space, and the value assigned to the leaf is the prediction for that region.\n",
        "*   The decision boundaries are hyperplanes perpendicular to the feature axes.\n",
        "\n",
        "**6. Overfitting and Regularization**\n",
        "\n",
        "*   Decision trees are prone to overfitting, where they learn the training data too well, including noise, and don't generalize well to new data.\n",
        "*   To prevent this, techniques like pruning are used:\n",
        "    *   **Pre-pruning:** Stop growing the tree early based on criteria like maximum depth or minimum samples per leaf.\n",
        "    *   **Post-pruning:** Grow the tree fully and then remove branches that don't improve performance on a validation set.\n",
        "\n",
        "**7. Mathematical Formulation of Pruning**\n",
        "\n",
        "*   Pruning can be formulated as an optimization problem, where we want to find the subtree that minimizes a cost function that balances the tree size and the misclassification error.\n",
        "*   This can involve concepts like cost complexity pruning, which introduces a penalty for larger trees.\n",
        "\n",
        "**Key Mathematical Tools Used**\n",
        "\n",
        "*   **Probability and Statistics:** Proportions, probabilities, entropy calculations.\n",
        "*   **Logarithms:** Used in entropy calculation.\n",
        "*   **Linear Algebra:** Representing data and decision boundaries.\n",
        "*   **Optimization:** Finding the best splits and pruning the tree.\n",
        "\n",
        "**In essence, decision trees are built using a combination of information theory (entropy, information gain), statistical measures (proportions), and optimization techniques to find the best way to partition the data and make predictions.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jo--J7Yc9NfN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7-b5iid9Puh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-pruning, also known as early stopping, is a technique used in decision trees to prevent overfitting. Overfitting occurs when the tree becomes too complex and learns the training data too well, including noise and outliers. This leads to poor performance on unseen data.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "Pre-pruning involves stopping the growth of the decision tree *before* it reaches its full potential. This is done by setting certain constraints or thresholds that limit the tree's depth and complexity. The tree stops splitting further when any of these conditions are met.\n",
        "\n",
        "**Common Pre-pruning Techniques:**\n",
        "\n",
        "*   **Maximum Depth:** Limits the maximum depth of the tree. Once the tree reaches this depth, it stops growing, even if further splits could potentially improve accuracy on the training data.\n",
        "*   **Minimum Samples per Leaf:** Sets a minimum number of samples required to be in a leaf node. If a split would result in a leaf node with fewer samples than this threshold, the split is not performed.\n",
        "*   **Minimum Samples per Split:** Specifies the minimum number of samples required to split an internal node. If a node has fewer samples than this threshold, it is not split further.\n",
        "*   **Maximum Features:** Limits the number of features considered for splitting at each node. This can help to prevent the tree from becoming too complex and also speed up the training process.\n",
        "\n",
        "**Advantages of Pre-pruning:**\n",
        "\n",
        "*   **Reduces Overfitting:** By stopping the tree's growth early, it avoids capturing noise and outliers in the training data, leading to better generalization on unseen data.\n",
        "*   **Faster Training:** Pre-pruning results in smaller trees, which are faster to train and easier to interpret.\n",
        "\n",
        "**Disadvantages of Pre-pruning:**\n",
        "\n",
        "*   **Risk of Underfitting:** If the stopping criteria are too strict, the tree might be too shallow to capture important patterns in the data, leading to underfitting. Underfitting occurs when the model is too simple to learn the underlying structure of the data, resulting in poor performance on both training and unseen data.\n",
        "\n",
        "**Key Considerations:**\n",
        "\n",
        "*   The effectiveness of pre-pruning depends on carefully tuning the hyperparameters that control the stopping criteria. These hyperparameters are often determined using techniques like cross-validation.\n",
        "*   Pre-pruning is a trade-off between preventing overfitting and potentially underfitting the data. It's important to find the right balance to achieve good performance on unseen data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WdRsDCej9QC_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tkvZC_CN9SY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. What is Post-Pruning in Decision Trees?\n",
        "\n",
        "Post-pruning, also known as backward pruning, is another technique used in decision trees to prevent overfitting. Unlike pre-pruning, which stops the tree from growing *before* it gets too complex, post-pruning allows the tree to grow fully and then *prunes* or removes branches that do not contribute significantly to the model's performance.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1.  **Grow the full tree:** The decision tree is first grown to its maximum possible extent, often resulting in a very complex tree that may overfit the training data.\n",
        "\n",
        "2.  **Prune branches:** The algorithm then traverses the tree from the bottom up (from the leaf nodes towards the root). At each internal node, it considers removing the subtree rooted at that node.\n",
        "\n",
        "3.  **Evaluate performance:** To decide whether to prune a branch, the algorithm evaluates the performance of the tree *with* and *without* that branch. This evaluation is typically done using a validation set (a portion of the training data that is held back during the initial tree construction).\n",
        "\n",
        "4.  **Remove branches:** If removing the branch improves or at least doesn't significantly degrade the performance on the validation set, the branch is pruned (replaced by a leaf node).  This process continues recursively up the tree.\n",
        "\n",
        "**Common Post-pruning Techniques:**\n",
        "\n",
        "*   **Reduced Error Pruning:** This method removes nodes if the error rate on the validation set improves after the removal.\n",
        "*   **Cost Complexity Pruning (also called Weakest Link Pruning):** This method introduces a cost complexity parameter (alpha) that penalizes the tree for having too many nodes. The algorithm finds the subtree that minimizes a combination of the error rate and the tree size (number of leaves).  Different values of alpha lead to different levels of pruning.\n",
        "\n",
        "**Advantages of Post-pruning:**\n",
        "\n",
        "*   **More Accurate Pruning:** Because post-pruning considers the performance of subtrees on a validation set, it can often make more informed decisions about which branches to prune compared to pre-pruning.  It can identify and remove branches that might seem important based on the training data alone but don't generalize well.\n",
        "*   **Less Prone to Underfitting:** Since the tree is initially grown fully, post-pruning is less likely to underfit the data compared to pre-pruning.  It has a chance to capture complex relationships before pruning back unnecessary complexity.\n",
        "\n",
        "**Disadvantages of Post-pruning:**\n",
        "\n",
        "*   **Computationally More Expensive:** Post-pruning requires growing the full tree first and then evaluating the performance of many subtrees, making it computationally more intensive than pre-pruning.\n",
        "*   **Requires a Validation Set:** Post-pruning needs a separate validation set to evaluate the performance of the tree during the pruning process.  This reduces the amount of data available for training the initial (full) tree.\n",
        "\n",
        "**Key Considerations:**\n",
        "\n",
        "*   The choice between post-pruning techniques (like Reduced Error Pruning or Cost Complexity Pruning) and the optimal value of any associated parameters (like alpha in Cost Complexity Pruning) can be important for the effectiveness of post-pruning.  Cross-validation is often used to tune these parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "NBuTJgd79S10"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sE9jSlrl9U9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. What is the difference between Pre-Pruning and Post-Pruning?\n",
        "\n",
        "Both pre-pruning and post-pruning are techniques used to prevent overfitting in decision trees, but they approach the problem from different directions. Here's a breakdown of their key differences:\n",
        "\n",
        "**Pre-pruning (Early Stopping)**\n",
        "\n",
        "*   **When it happens:** Stops the tree from growing *before* it gets too complex.\n",
        "*   **How it works:** Sets constraints on the tree's growth, such as:\n",
        "    *   Maximum depth of the tree\n",
        "    *   Minimum number of samples required in a leaf node\n",
        "    *   Minimum number of samples required to split a node\n",
        "*   **Advantages:**\n",
        "    *   Simpler and faster to implement.\n",
        "    *   Reduces overfitting by preventing the tree from learning noise in the training data.\n",
        "*   **Disadvantages:**\n",
        "    *   Can lead to underfitting if the constraints are too strict. The tree might stop growing before capturing important patterns.\n",
        "    *   It's challenging to determine the optimal stopping criteria beforehand.\n",
        "\n",
        "**Post-pruning (Backward Pruning)**\n",
        "\n",
        "*   **When it happens:** Grows the tree fully and then *prunes* or removes unnecessary branches.\n",
        "*   **How it works:**\n",
        "    *   The tree is grown to its maximum extent, often overfitting the training data.\n",
        "    *   The algorithm then traverses the tree from the bottom up.\n",
        "    *   At each node, it considers removing the subtree rooted at that node.\n",
        "    *   The decision to prune is based on evaluating the tree's performance on a validation set.\n",
        "*   **Advantages:**\n",
        "    *   Generally more accurate than pre-pruning because it considers the performance of subtrees on a validation set.\n",
        "    *   Less prone to underfitting as the tree is initially grown fully.\n",
        "*   **Disadvantages:**\n",
        "    *   Computationally more expensive as it requires growing the full tree first.\n",
        "    *   Needs a separate validation set, which reduces the data available for training.\n",
        "\n",
        "**Here's a table summarizing the key differences:**\n",
        "\n",
        "| Feature | Pre-pruning | Post-pruning |\n",
        "|---|---|---|\n",
        "| **Timing** | Stops growth early | Prunes after full growth |\n",
        "| **Approach** | Sets constraints on growth | Removes branches |\n",
        "| **Accuracy** | Can be less accurate due to underfitting | Generally more accurate |\n",
        "| **Computational cost** | Less expensive | More expensive |\n",
        "| **Data requirements** | Doesn't need a validation set | Requires a validation set |\n",
        "| **Overfitting risk** | Lower | Lower (after pruning) |\n",
        "| **Underfitting risk** | Higher | Lower |\n",
        "\n",
        "**Which one to use?**\n",
        "\n",
        "*   Pre-pruning is often used as a quick and simple way to prevent overfitting, especially when dealing with large datasets where computational cost is a concern.\n",
        "*   Post-pruning is generally preferred when accuracy is more important and computational resources are available. It often leads to better performance as it makes more informed decisions about pruning based on validation set performance.\n",
        "\n",
        "Ultimately, the choice between pre-pruning and post-pruning depends on the specific dataset, the goals of the analysis, and the available computational resources. In many cases, post-pruning is the preferred method due to its higher accuracy, but pre-pruning can be a useful starting point or a good option when speed is a primary concern.\n"
      ],
      "metadata": {
        "id": "Uiy-5BsR9VU5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TVLGAjR9XZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. What is a Decision Tree Regressor?\n",
        "\n",
        "A Decision Tree Regressor is a type of decision tree specifically designed for regression tasks. Unlike a Decision Tree Classifier, which predicts categorical outcomes (classes), a Decision Tree Regressor predicts continuous outcomes (numerical values).\n",
        "\n",
        "Here's a breakdown of how it works:\n",
        "\n",
        "**1. Building the Tree (Training Phase):**\n",
        "\n",
        "*   **Data Preparation:** The algorithm starts with a dataset of labeled examples, where each example has input features and a corresponding continuous target value.\n",
        "*   **Feature Selection:** The core of the algorithm is how it chooses which feature to split on at each node.  It aims to select the feature that best separates the data into subsets that are more homogeneous with respect to the target variable (i.e., the target values within each subset are as similar as possible). Common criteria for feature selection in regression include:\n",
        "    *   **Mean Squared Error (MSE):** Measures the average squared difference between the predicted values and the actual values. The goal is to minimize the MSE.\n",
        "    *   **Mean Absolute Error (MAE):** Measures the average absolute difference between the predicted values and the actual values. The goal is to minimize the MAE.\n",
        "*   **Splitting:** Once a feature is selected, the algorithm finds an optimal threshold to split the data.  For numerical features, this threshold divides the data into two subsets (e.g., values less than or equal to the threshold go down one branch, and values greater than the threshold go down the other). For categorical features, each category typically gets its own branch.\n",
        "*   **Recursion:** This process of feature selection and splitting is repeated recursively for each subset until a stopping criterion is met. Common stopping criteria include:\n",
        "    *   A maximum tree depth is reached.\n",
        "    *   A minimum number of samples are required at a leaf node.\n",
        "    *   No more features are available to split on.\n",
        "*   **Leaf Node Assignment:** When a leaf node is reached, it's assigned a value.  For regression, this is typically the average (or median) of the target values of the samples in that leaf.\n",
        "\n",
        "**2. Making Predictions (Testing Phase):**\n",
        "\n",
        "*   **Input:** Given a new, unseen data point with its features.\n",
        "*   **Traversal:** The algorithm starts at the root node of the decision tree.\n",
        "*   **Decision at Each Node:** At each internal node, the algorithm checks the value of the corresponding feature in the input data point. It then follows the branch corresponding to that value (based on the splitting threshold learned during training).\n",
        "*   **Leaf Node:** This process continues until a leaf node is reached.\n",
        "*   **Output:** The average (or median) target value of the samples in that leaf node is the predicted outcome for the input data point.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vbEEVJOI9X0W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UwC5-TE59Z2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. What are the advantages and disadvantages of Decision Trees?\n",
        "\n",
        "Decision Trees, whether used for classification or regression, have a set of advantages and disadvantages that make them suitable for some tasks but less so for others. Here's a breakdown:\n",
        "\n",
        "**Advantages of Decision Trees:**\n",
        "\n",
        "*   **Easy to Understand and Interpret:** Decision trees are highly intuitive and can be visualized easily. This makes them \"white box\" models, meaning you can understand exactly how the model arrives at its predictions. This interpretability is a significant advantage in many applications.\n",
        "*   **Handles Both Categorical and Numerical Data:** Decision trees can work with both types of data without requiring extensive preprocessing. This versatility simplifies the data preparation process.\n",
        "*   **Non-linear Relationships:** They can capture non-linear relationships between features and the target variable, unlike simpler models like linear regression. This makes them suitable for more complex datasets.\n",
        "*   **Feature Importance:** Decision trees provide insights into which features are most important for making predictions. This information can be valuable for feature selection and understanding the underlying data.\n",
        "*   **No Assumptions about Data Distribution:** Unlike some other algorithms, decision trees don't make strong assumptions about the distribution of the data. This makes them more robust to outliers and non-normally distributed data.\n",
        "*   **Simple to Implement:** The basic algorithm is relatively straightforward to implement.\n",
        "\n",
        "**Disadvantages of Decision Trees:**\n",
        "\n",
        "*   **Overfitting:** Decision trees are prone to overfitting, especially if they are allowed to grow too deep. Overfitting means the model memorizes the training data, including noise and outliers, and performs poorly on unseen data.\n",
        "*   **Sensitivity to Data:** Small changes in the training data can sometimes lead to very different tree structures. This instability can be a problem if the data is noisy or if the model needs to be very robust.\n",
        "*   **Bias towards Features with Many Levels:** Features with many possible values (high cardinality) can sometimes be unfairly favored during feature selection. This can lead to imbalanced trees and potentially poor performance.\n",
        "*   **Tendency to Create Complex Trees:** Without proper pruning or constraints, decision trees can become very large and complex, making them difficult to interpret and prone to overfitting.\n",
        "*   **Not Always the Most Accurate:** While decision trees are versatile, they are not always the most accurate models. Other algorithms, like ensemble methods (e.g., Random Forests, Gradient Boosting), often achieve better performance, especially on complex datasets.\n",
        "*   **Can Create Unrealistic Decision Boundaries:** Decision trees create decision boundaries that are perpendicular to the feature axes. This can sometimes lead to unrealistic or suboptimal decision boundaries, especially if the underlying relationships are not axis-aligned.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tWX5bN-n9aRH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "clQ2QzzT9cee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. How does a Decision Tree handle missing values?\n",
        "\n",
        "\n",
        "Decision trees can handle missing values in several ways, and the specific approach depends on the implementation and the algorithm used. Here are the most common strategies:\n",
        "\n",
        "**1. Ignoring Missing Values:**\n",
        "\n",
        "*   **During Training:**  The simplest approach is to ignore instances with missing values during the tree construction process.  The algorithm simply skips over these instances when calculating impurity measures and making splitting decisions.\n",
        "*   **During Prediction:**  If a missing value is encountered during prediction, the instance is often assigned to the most frequent branch at that node or a branch chosen based on surrogate splits (explained below).\n",
        "\n",
        "**2. Imputation:**\n",
        "\n",
        "*   **During Training:** Missing values can be filled in (imputed) with estimated values before or during the tree construction. Common imputation methods include:\n",
        "    *   **Mean/Median Imputation:** Replacing missing numerical values with the mean or median of the observed values for that feature.\n",
        "    *   **Mode Imputation:** Replacing missing categorical values with the most frequent category.\n",
        "    *   **K-Nearest Neighbors Imputation (KNN Imputation):** Filling in missing values based on the values of the most similar instances.\n",
        "*   **During Prediction:**  If imputation was used during training, the same imputation method is applied to missing values encountered during prediction.\n",
        "\n",
        "**3. Surrogate Splits:**\n",
        "\n",
        "*   This is a more sophisticated approach.  When a node is split on a particular feature, the algorithm also identifies \"surrogate splits\" on other features that have similar splitting behavior.  If an instance has a missing value for the primary splitting feature, the surrogate split is used instead.  The best surrogate split is the one that most closely mimics the primary split.  This allows the tree to still make a decision even with missing data.\n",
        "\n",
        "**4. Handling Missing Values as a Separate Category:**\n",
        "\n",
        "*   For categorical features, missing values can be treated as a separate, distinct category. This can be useful if the fact that a value is missing is itself informative.\n",
        "\n",
        "**5. Probabilistic Methods:**\n",
        "\n",
        "*   Some algorithms use probabilistic methods to handle missing values.  Instead of assigning an instance to a single branch, they might assign probabilities to different branches based on the distribution of values in the training data.\n",
        "\n",
        "**Which Method is Best?**\n",
        "\n",
        "There's no single \"best\" method. The optimal strategy depends on the specific dataset, the amount of missing data, and the nature of the missing values (e.g., are they missing completely at random, or is there a pattern to the missingness?).\n",
        "\n",
        "*   Ignoring missing values is simple but can lead to information loss, especially if there's a lot of missing data.\n",
        "*   Imputation can introduce bias if not done carefully.  Simple methods like mean/median/mode imputation can distort the data distribution.\n",
        "*   Surrogate splits are a more robust approach but add computational complexity.\n",
        "*   Treating missing values as a separate category can be useful if the missingness itself is meaningful.\n",
        "\n",
        "**In Practice:**\n",
        "\n",
        "Most machine learning libraries that implement decision trees (like scikit-learn in Python) provide options for handling missing values.  Often, they use a combination of these techniques, such as surrogate splits or imputation. It's often a good idea to experiment with different strategies and evaluate their impact on the model's performance using techniques like cross-validation.  Understanding how your chosen library handles missing values is crucial for getting the best results from your decision tree model.\n"
      ],
      "metadata": {
        "id": "jDGtNLlH9c3E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMo0q31s9ev7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. How does a Decision Tree handle categorical features?\n",
        "\n",
        "Decision trees handle categorical features in a straightforward way, making them quite versatile. Here's how it works:\n",
        "\n",
        "**1. Splitting on Categorical Features:**\n",
        "\n",
        "*   **Creating Branches:** When a decision tree encounters a categorical feature, it typically creates a separate branch for each category.  For example, if the feature is \"color\" with categories \"red,\" \"blue,\" and \"green,\" the tree will have three branches stemming from the node where this feature is used for splitting.\n",
        "*   **Determining the Best Split:** The decision tree algorithm evaluates all possible splits on the categorical feature.  It calculates the impurity measure (Gini impurity or entropy for classification, MSE or MAE for regression) for each possible split and chooses the split that minimizes the impurity.\n",
        "*   **Example:** Imagine you're predicting customer churn, and one feature is \"subscription type\" with categories \"basic,\" \"premium,\" and \"enterprise.\" The tree might split on this feature, creating three branches, one for each subscription type. The algorithm would determine which split (or combination of splits if handling multi-valued categorical features) results in the greatest information gain or lowest impurity.\n",
        "\n",
        "**2. Handling Multi-valued Categorical Features (High Cardinality):**\n",
        "\n",
        "*   Categorical features with many categories (high cardinality) can pose a challenge.  If each category gets its own branch, the tree can become very wide and prone to overfitting.  Several strategies are used to address this:\n",
        "    *   **Grouping Categories:**  Categories can be grouped together based on their similarity or their impact on the target variable.  For instance, less frequent categories might be combined into an \"other\" category.\n",
        "    *   **Binary Splits:** Instead of creating a branch for each category, the algorithm can find the best binary split.  For example, it might find that the best split is to group \"red\" and \"green\" together versus \"blue.\" This reduces the number of branches and makes the tree less complex.\n",
        "    *   **Feature Encoding (for some algorithms):** Certain decision tree implementations may use one-hot encoding or other encoding schemes for categorical features, especially if they are integrated with other machine learning algorithms that require numerical input. However, the core decision tree algorithm itself can directly handle categorical data without encoding.\n",
        "\n",
        "**3.  Missing Values in Categorical Features:**\n",
        "\n",
        "*   Missing values in categorical features can be handled in a few ways:\n",
        "    *   **Treat as a Separate Category:** The most common approach is to treat missing values as a separate, distinct category. This can be useful if the fact that a value is missing is itself informative.\n",
        "    *   **Imputation:** Missing values can be imputed using methods like mode imputation (replacing with the most frequent category).\n",
        "    *   **Surrogate Splits:**  As mentioned before, if the tree uses surrogate splits, the split on a surrogate feature can be used when the primary feature has a missing value.\n",
        "\n",
        "**Key Considerations:**\n",
        "\n",
        "*   **Computational Cost:**  Splitting on categorical features can be computationally more expensive than splitting on numerical features, especially for high-cardinality features.\n",
        "*   **Overfitting:**  High-cardinality categorical features can increase the risk of overfitting. Techniques like grouping categories or using binary splits can help mitigate this.\n",
        "\n",
        "**In summary:** Decision trees can directly handle categorical features by creating branches for each category. Strategies like grouping categories, binary splits, and treating missing values as a separate category are used to address the challenges posed by high-cardinality features and missing data.  This direct handling of categorical data is one of the advantages of decision trees, as it avoids the need for extensive feature engineering or encoding in many cases.\n"
      ],
      "metadata": {
        "id": "ttX87T-v9fSY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oCbUc7ig9hA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. What are some real-world applications of Decision Trees?\n",
        "\n",
        "Decision trees are incredibly versatile and find applications in a wide range of fields. Here are some real-world examples:\n",
        "\n",
        "**1. Healthcare:**\n",
        "\n",
        "*   **Diagnosis:** Doctors can use decision trees to diagnose diseases based on symptoms and patient history. For example, a decision tree might help determine the likelihood of a patient having a specific condition based on their age, blood pressure, and other relevant factors.\n",
        "*   **Risk Assessment:** Decision trees can be used to identify patients at high risk of developing certain conditions, allowing for early intervention and preventative care.\n",
        "*   **Treatment Selection:** Decision trees can assist in choosing the most appropriate treatment plan for a patient based on their individual characteristics and the specifics of their condition.\n",
        "\n",
        "**2. Finance:**\n",
        "\n",
        "*   **Credit Scoring:** Banks and financial institutions use decision trees to assess the creditworthiness of loan applicants. Factors like income, credit history, and employment status are used to predict the likelihood of a borrower defaulting on a loan.\n",
        "*   **Fraud Detection:** Decision trees can help identify potentially fraudulent transactions by analyzing patterns in transaction data. Unusual transaction amounts, locations, or frequencies can be flagged for further investigation.\n",
        "*   **Investment Decisions:** Decision trees can be used to model potential investment outcomes based on various market conditions and factors, helping investors make informed decisions.\n",
        "\n",
        "**3. Marketing:**\n",
        "\n",
        "*   **Customer Segmentation:** Decision trees can help segment customers into different groups based on their demographics, purchasing behavior, and other characteristics. This allows businesses to tailor their marketing campaigns to specific customer segments.\n",
        "*   **Targeted Advertising:** By understanding customer preferences and behaviors, decision trees can help businesses deliver more targeted and effective advertisements.\n",
        "*   **Customer Retention:** Decision trees can be used to predict which customers are likely to churn (cancel their service) so that businesses can take proactive steps to retain them.\n",
        "\n",
        "**4. Business:**\n",
        "\n",
        "*   **Decision Making:** Decision trees can be used to evaluate different business strategies and make informed decisions about product development, pricing, and resource allocation.\n",
        "*   **Risk Management:** Decision trees can help businesses assess and manage risks by modeling potential outcomes and their associated probabilities.\n",
        "*   **Operations Management:** Decision trees can be used to optimize processes and improve efficiency in areas like manufacturing, logistics, and supply chain management.\n",
        "\n",
        "**5. Other Applications:**\n",
        "\n",
        "*   **Environmental Science:** Decision trees can be used to model and predict environmental phenomena like weather patterns, pollution levels, and species distribution.\n",
        "*   **Education:** Decision trees can help educators identify students who are at risk of falling behind and provide them with targeted support.\n",
        "*   **Engineering:** Decision trees can be used in engineering design to optimize product performance and reliability.\n",
        "\n",
        "These are just a few examples of the many real-world applications of decision trees. Their versatility, interpretability, and ability to handle both categorical and numerical data make them a valuable tool in a wide range of fields.\n"
      ],
      "metadata": {
        "id": "ncwywTAr9hWn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "faXzASgl9iaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "c0mcSrILBF9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing requrid lib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7e99boPbDVaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.\n"
      ],
      "metadata": {
        "id": "X9XpKYSvBKDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data=load_iris()\n",
        "x=data.data\n",
        "y=data.target\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "dt=DecisionTreeClassifier()\n",
        "dt.fit(x_train,y_train)\n",
        "y_pred=dt.predict(x_test)\n",
        "print(\"accuracy is \",accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUVGlEKIEjVh",
        "outputId": "cd642b48-8eb5-4517-a5d8-aa1c2ff21e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy is  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n"
      ],
      "metadata": {
        "id": "lrf6pDuFCqDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_gini = DecisionTreeClassifier(criterion='gini')\n",
        "clf_gini.fit(x_train, y_train)\n",
        "print(\" Feature Importances (Gini):\", clf_gini.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8O4DxtHCtAY",
        "outputId": "3de1d563-65ca-463d-e02b-c0b5815355fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Feature Importances (Gini): [0.01667014 0.01667014 0.38926487 0.57739485]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy.\n"
      ],
      "metadata": {
        "id": "rzOu_g0kCtXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_entropy = DecisionTreeClassifier(criterion='entropy')\n",
        "clf_entropy.fit(x_train, y_train)\n",
        "y_pred_entropy = clf_entropy.predict(x_test)\n",
        "print(\" Accuracy (Entropy):\", accuracy_score(y_test, y_pred_entropy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ-4XQ4wCwd5",
        "outputId": "9f8b9447-15c2-47ec-dc87-10d0a4612d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy (Entropy): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE).\n"
      ],
      "metadata": {
        "id": "OKrtJziaCw2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(housing.data, housing.target, test_size=0.3, random_state=42)\n",
        "reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train_reg, y_train_reg)\n",
        "y_pred_reg = reg.predict(X_test_reg)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "print(\" MSE:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wlPMwzKFJzm",
        "outputId": "f237d639-1cff-4a0d-cb9d-e60fa4439022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " MSE: 0.5229620809892441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz.\n"
      ],
      "metadata": {
        "id": "fNorKWOPC0ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP2tiaduGGJx",
        "outputId": "f31a3949-f150-41f1-c0cd-941c4644952a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target # Target variable\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Export the decision tree to a DOT format string\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create a graph from the DOT format string\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "\n",
        "graph.view()\n",
        "\n",
        "#  save the graph to a file (e.g., as a PNG or PDF):\n",
        "graph.render(\"iris_tree\")  # Saves as iris_tree.png (by default)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "U58xrPavC3Rf",
        "outputId": "e4bc1881-c3ee-4692-c84b-3e11849d22d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'iris_tree.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.\n"
      ],
      "metadata": {
        "id": "M3CUDpiiC3sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a fully grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Accuracy of fully grown tree:\", accuracy_full)\n",
        "print(\"Accuracy of tree with max_depth=3:\", accuracy_depth3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5otb05q3C6Jj",
        "outputId": "74dbf905-1303-46a6-e2d7-876c78737a61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully grown tree: 1.0\n",
            "Accuracy of tree with max_depth=3: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree.\n"
      ],
      "metadata": {
        "id": "CAQOHO4-C6g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with default parameters (fully grown)\n",
        "clf_default = DecisionTreeClassifier()\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Train a Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples = DecisionTreeClassifier(min_samples_split=5)\n",
        "clf_min_samples.fit(X_train, y_train)\n",
        "y_pred_min_samples = clf_min_samples.predict(X_test)\n",
        "accuracy_min_samples = accuracy_score(y_test, y_pred_min_samples)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Accuracy of default tree:\", accuracy_default)\n",
        "print(\"Accuracy of tree with min_samples_split=5:\", accuracy_min_samples)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpTabrxHC9us",
        "outputId": "20f4ad72-d9bc-4d95-9ab8-b1ab3cca393e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of default tree: 1.0\n",
            "Accuracy of tree with min_samples_split=5: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data.\n"
      ],
      "metadata": {
        "id": "OkbbYu43C-Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with default parameters (fully grown)\n",
        "clf_default = DecisionTreeClassifier()\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Train a Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples = DecisionTreeClassifier(min_samples_split=5)\n",
        "clf_min_samples.fit(X_train, y_train)\n",
        "y_pred_min_samples = clf_min_samples.predict(X_test)\n",
        "accuracy_min_samples = accuracy_score(y_test, y_pred_min_samples)\n",
        "\n",
        "# Print the accuracies\n",
        "print(\"Accuracy of default tree:\", accuracy_default)\n",
        "print(\"Accuracy of tree with min_samples_split=5:\", accuracy_min_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHoiDCv-DAfa",
        "outputId": "ed85cfa3-a83e-44b9-d259-1695d0e11e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of default tree: 1.0\n",
            "Accuracy of tree with min_samples_split=5: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification.\n"
      ],
      "metadata": {
        "id": "uRhZLlhGDA7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris  # Or any other multiclass dataset\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a multiclass dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target  # Iris has 3 classes\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier (you can customize this)\n",
        "base_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Create the OneVsRestClassifier\n",
        "ovr_classifier = OneVsRestClassifier(base_classifier)\n",
        "\n",
        "# Train the OneVsRestClassifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_ovr = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(\"One-vs-Rest Accuracy:\", accuracy_ovr)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwBeM8z8DDfw",
        "outputId": "4cedc8fa-8c02-47ea-cd83-8516a1061156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-Rest Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores.\n"
      ],
      "metadata": {
        "id": "7vSvZUjBDD5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)  # Train on the training data\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"{iris.feature_names[i]}: {importance}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTcnjremDGTL",
        "outputId": "671959e4-fbb0-44f2-9d90-44ac24829bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.01911001911001911\n",
            "sepal width (cm): 0.0\n",
            "petal length (cm): 0.4235665821031675\n",
            "petal width (cm): 0.5573233987868135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree.\n"
      ],
      "metadata": {
        "id": "FfMS-biSDGyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an unrestricted Decision Tree Regressor (default parameters)\n",
        "reg_unrestricted = DecisionTreeRegressor()\n",
        "reg_unrestricted.fit(X_train, y_train)\n",
        "y_pred_unrestricted = reg_unrestricted.predict(X_test)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "# Train a Decision Tree Regressor with max_depth=5\n",
        "reg_depth5 = DecisionTreeRegressor(max_depth=5)\n",
        "reg_depth5.fit(X_train, y_train)\n",
        "y_pred_depth5 = reg_depth5.predict(X_test)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "\n",
        "# Print the Mean Squared Errors (MSE)\n",
        "print(\"MSE of unrestricted tree:\", mse_unrestricted)\n",
        "print(\"MSE of tree with max_depth=5:\", mse_depth5)\n",
        "\n",
        "# Compare the MSEs\n",
        "if mse_depth5 < mse_unrestricted:\n",
        "    print(\"The tree with max_depth=5 has lower MSE (better performance).\")\n",
        "elif mse_depth5 > mse_unrestricted:\n",
        "    print(\"The unrestricted tree has lower MSE (better performance).\")\n",
        "else:\n",
        "    print(\"Both trees have the same MSE.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLQo5t0MDJAp",
        "outputId": "0097ad63-ec3b-4050-ee14-bd21ddb8d6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE of unrestricted tree: 0.533622842045478\n",
            "MSE of tree with max_depth=5: 0.5210801561811793\n",
            "The tree with max_depth=5 has lower MSE (better performance).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.\n"
      ],
      "metadata": {
        "id": "K19S8LvMDJfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier (you can customize this)\n",
        "clf = DecisionTreeClassifier(random_state=0)  # Add random_state for reproducibility\n",
        "\n",
        "# Get the cost complexity pruning path\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas  # Different alpha values for pruning\n",
        "impurities = path.impurities  # Impurities corresponding to alpha values\n",
        "\n",
        "# Store accuracy scores for different alpha values\n",
        "accuracies_train = []\n",
        "accuracies_test = []\n",
        "\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=0) # Use the same random state\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "\n",
        "    y_pred_train = clf_pruned.predict(X_train)\n",
        "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
        "    accuracies_train.append(accuracy_train)\n",
        "\n",
        "    y_pred_test = clf_pruned.predict(X_test)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "    accuracies_test.append(accuracy_test)\n",
        "\n",
        "\n",
        "# Visualize the effect of CCP on accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, accuracies_train, marker='o', label='Training Accuracy')\n",
        "plt.plot(ccp_alphas, accuracies_test, marker='x', label='Testing Accuracy')\n",
        "plt.xlabel('CCP Alpha')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of Cost Complexity Pruning on Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "PgxyBp3hDMlQ",
        "outputId": "9a30e84a-1289-41f2-f05f-c232613d784a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score.\n"
      ],
      "metadata": {
        "id": "AzQ1_KBdDNCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # random_state for reproducibility\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "# Important: Specify the 'average' parameter for multiclass classification\n",
        "precision = precision_score(y_test, y_pred, average='weighted') # 'weighted' averages by support\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn9Ao0HdDPEx",
        "outputId": "258937f9-b160-4376-93d8-791a9feac7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn.\n"
      ],
      "metadata": {
        "id": "ZhiUuhH7DPdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size for better visualization\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,  # Customize heatmap\n",
        "            xticklabels=iris.target_names, yticklabels=iris.target_names) # Add class labels\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "kWwKsCYdDRjw",
        "outputId": "dfe33d1b-e2ad-41de-fe6c-804dd036f8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split."
      ],
      "metadata": {
        "id": "KxyLC9gTDSDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': range(1, 11),  # Try max_depth from 1 to 10 (inclusive)\n",
        "    'min_samples_split': range(2, 11) # Try min_samples_split from 2 to 10 (inclusive)\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object.  cv=5 means 5-fold cross-validation.\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy') # scoring='accuracy' for classification\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters found\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Print the best accuracy score achieved\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# --- Evaluate the model with the best hyperparameters on the test set ---\n",
        "# Get the best estimator (the trained classifier with the best parameters)\n",
        "best_clf = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_best = best_clf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
        "print(\"Test Accuracy with Best Hyperparameters:\", test_accuracy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Iu0ro-1DTFd",
        "outputId": "71c34022-c547-42b2-b3b0-3e7b5fac5a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 4, 'min_samples_split': 6}\n",
            "Best Accuracy: 0.9428571428571428\n",
            "Test Accuracy with Best Hyperparameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gwXRT2-uJk_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}